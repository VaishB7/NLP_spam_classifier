Assignment 1 Report

Name: VAISHNAVI BALACHANDRAN (USC ID: 6286277121)

1. Performance on the development data with 100% of the training data
1a. spam precision: 0.98
1b. spam recall: 0.98
1c. spam F1 score: 0.98
1d. ham precision:0.95
1e. ham recall: 0.94
1f. ham F1 score:0.95

2. Performance on the development data with 10% of the training data
2a. spam precision: 0.98
2b. spam recall: 0.76
2c. spam F1 score: 0.86
2d. ham precision:0.24
2e. ham recall: 0.87
2f. ham F1 score:0.38

3. Description of enhancement(s) you tried (e.g., different approach(es) to smoothing, treating common words differently, dealing with unknown words differently): 

Instead of Using Add-one Smoothing, I used JM Smoothing (Jelinek-Mercer Smoothing). According to an NLP paper, this technique is supposed to give better performance/accuracy over Add-one Smoothing. The Smoothing depends on λ, λ = 1 gives maximum smoothing. The formula for smoothing is as follows:

P(W|Category) = (1- λ)* [Count(W in that Category)/Count(Total Vocab in that Category)] + λ*P(W|Collection)

[In my case, I got a better result in classification by using λ = 0.5]

Generally, using JM Smoothing, it improves the accuracy but since in the first Part, I used Log to calculate probabilities, the raise in Accuracy is not that visible. 

As we can see, using JM Smoothing at λ = 0.5 gives a slightly better Precision & recall values over our data. 

4. Best performance results based on enhancements. Note that these could be the same or worse than the standard implementation.
4a. spam precision:0.98
4b. spam recall: 0.99
4c. spam F1 score: 0.99
4d. ham precision:0.98
4e. ham recall: 0.95
4f. ham F1 score:0.96



